{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e463d72f",
   "metadata": {},
   "source": [
    "# Q2.1 - Systematic Encoding Matrix  $G$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb050548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original H:\n",
      "[[1 1 1 1 0 0]\n",
      " [0 0 1 1 0 1]\n",
      " [1 0 0 1 1 0]]\n",
      "----------------------------------------\n",
      "System Parameters: N=6, M=3, K=3\n",
      "----------------------------------------\n",
      "\n",
      "Permuted and Row-Reduced H (H_hat = [P | I]):\n",
      "[[1 1 0 1 0 0]\n",
      " [1 1 1 0 1 0]\n",
      " [1 0 1 0 0 1]]\n",
      "\n",
      "Systematic Generator Matrix G (G = [I_K / P]):\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]\n",
      " [1 0 1]]\n",
      "\n",
      "Verification (H_hat @ G % 2):\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "All zero? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rref(M):\n",
    "    \"\"\"\n",
    "    Computes the Row Reduced Echelon Form (RREF) of matrix M using XOR.\n",
    "    Returns the RREF matrix and the list of pivot column indices.\n",
    "    \"\"\"\n",
    "    M = M.copy()\n",
    "    rows, cols = M.shape\n",
    "    pivot_cols = []\n",
    "    current_row = 0\n",
    "    \n",
    "    for j in range(cols):\n",
    "        if current_row >= rows:\n",
    "            break\n",
    "        \n",
    "        pivot_row = -1\n",
    "        for i in range(current_row, rows):\n",
    "            if M[i, j] == 1:\n",
    "                pivot_row = i\n",
    "                break\n",
    "        \n",
    "        if pivot_row != -1:\n",
    "            if pivot_row != current_row:\n",
    "                M[[current_row, pivot_row]] = M[[pivot_row, current_row]]\n",
    "            # Record the pivot column\n",
    "            pivot_cols.append(j)\n",
    "            \n",
    "            # Use XOR to zero out all other 1s in column\n",
    "            for i in range(rows):\n",
    "                if i != current_row and M[i, j] == 1:\n",
    "                    # M[i] = M[i] XOR M[current_row]\n",
    "                    M[i] = M[i] ^ M[current_row]\n",
    "            \n",
    "            current_row += 1\n",
    "            \n",
    "    return M, pivot_cols\n",
    "\n",
    "def build_systematic_encoder(H_original):\n",
    "    \"\"\"\n",
    "    Builds the systematic LDPC encoder G using the procedure H_hat = [P | I] and G = [I_K / P].\n",
    "    \"\"\"\n",
    "    H = H_original.copy()\n",
    "    M, N = H.shape\n",
    "    K = N - M  # K = message length\n",
    "    \n",
    "    # Compute RREF of H to find pivots\n",
    "    H_rref, pivot_cols = rref(H)\n",
    "\n",
    "    all_cols = list(range(N))\n",
    "    non_pivot_cols = [c for c in all_cols if c not in pivot_cols]\n",
    "    \n",
    "    # Determine Permutation Order: [Non-Pivots] (P-part) + [Pivots] (I-part)\n",
    "    perm_order = non_pivot_cols + pivot_cols\n",
    "    \n",
    "    # Apply Permutation to the RREF matrix to get the final H_hat = [P | I]\n",
    "    H_hat = H_rref[:, perm_order]\n",
    "\n",
    "    P = H_hat[:, :K]  # P is the first K columns of H_hat\n",
    "    I_K = np.eye(K, dtype=int)\n",
    "    G = np.vstack([I_K, P])\n",
    "    \n",
    "    return H_hat, G\n",
    "\n",
    "\n",
    "# Test Input\n",
    "H_matrix = np.array([\n",
    "    [1, 1, 1, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 1],\n",
    "    [1, 0, 0, 1, 1, 0]\n",
    "], dtype=int)\n",
    "\n",
    "print(\"Original H:\")\n",
    "print(H_matrix)\n",
    "\n",
    "# Run the function\n",
    "H_hat, G = build_systematic_encoder(H_matrix)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"System Parameters: N=6, M=3, K=3\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nPermuted and Row-Reduced H (H_hat = [P | I]):\")\n",
    "print(H_hat)\n",
    "\n",
    "print(\"\\nSystematic Generator Matrix G (G = [I_K / P]):\")\n",
    "print(G)\n",
    "\n",
    "# Verification: H_hat * G should be 0\n",
    "check = (H_hat @ G) % 2\n",
    "print(\"\\nVerification (H_hat @ G % 2):\")\n",
    "print(check)\n",
    "print(f\"All zero? {np.all(check == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f332ed1",
   "metadata": {},
   "source": [
    "---\n",
    "We must ensure $H$ is in RREF form ($H_{RREF} = [ P \\ | \\ I ]$) to solve the system of linear equations. This explicitly separates the message bits from the parity bits, allowing us to directly construct $G$ without doing any further algebra. Example $H$ in the question is not in RREF. So, we have implemented a function to convert $H$ to $H_{RREF}$\n",
    "\n",
    "We verify this orthogonality (that $H_{RREF} \\times G = 0$) below:\n",
    "\n",
    "$H_{RREF} = [ P \\ | \\ I ]$\n",
    "\n",
    "$H_{RREF} \\times G = [ P \\ | \\ I ] \\times \\begin{bmatrix} I \\\\ P \\end{bmatrix}$\n",
    "\n",
    "   $= (P \\times I) + (I \\times P)$\n",
    "\n",
    "   $= P + P = 0$ (since we are operating in binary where $x+x=0$)\n",
    "\n",
    "we must have our $\\hat{H}$ be equal to $H_{RREF}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1deef42",
   "metadata": {},
   "source": [
    "# Q2.2 - Factor Graph for Matrix $H$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c8225",
   "metadata": {},
   "source": [
    "<img src=\"Images/factorGraph.png\" width=\"400\">\n",
    "\n",
    "Here is the factor graph from the H given in Question 2.1\n",
    "\n",
    "A factor graph for an LDPC code is a bipartite graph\n",
    "\n",
    "Each check node $c_i$ connects to variable node $x_j$ if the corresponding entry in the matrix row is 1 ($H_{ij} = 1$).\n",
    "\n",
    "$P(x) \\propto \\psi_1(x_1, x_2, x_3, x_4) \\cdot \\psi_2(x_3, x_4, x_6) \\cdot \\psi_3(x_1, x_4, x_5)$,\n",
    "\n",
    "$\n",
    "H =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac5dad",
   "metadata": {},
   "source": [
    "# Q2.3 - LDPC decoder\n",
    "\n",
    "The Sum-Product algorithm is implemented with Log-Likelihood Ratios (LLR) defined as $L = \\ln \\frac{P(x=0)}{P(x=1)}$.\n",
    "- $L > 0$ implies the bit is likely 0, and $L < 0$ implies the bit is likely 1.\n",
    "- The LLR channel for each bit $y_j$ is: $L_j^{channel} = (1 - 2y_j) \\ln\\left(\\frac{1-p}{p}\\right)$\n",
    "- The term $(1- 2y_j)$ ensures the sign is correct: positive if $y_j=0$ and negative if $y_j=1$.\n",
    "\n",
    "Since we are using positive and negative values, we implement $\\tanh$ for our updates.\n",
    "\n",
    "### Iterative Message Passing\n",
    "\n",
    "The decoding process iterates between two types of updates on the factor graph until convergence or a maximum iteration count is reached\n",
    "\n",
    "#### Check Node Update (Horizontal Step)\n",
    "The check nodes calculate the probability that a parity constraint is satisfied. In the LLR domain, the parity operation (XOR) is transformed into a product of hyperbolic tangents. \n",
    "For a check node $i$ connected to variable nodes $j$, the message $r_{i \\to j}$ is updated as: \n",
    "\n",
    "$r_{i \\to j} = 2 \\tanh^{-1}\\left( \\prod_{k \\in \\mathcal{N}(i) \\setminus j} \\tanh\\left(\\frac{q_{k \\to i}}{2}\\right) \\right)$\n",
    "\n",
    "This formula effectively calculates the \"soft XOR\" of all other connected bits. If the product is positive, the parity suggests bit $j$ should be 0; if negative, it suggests 1.\n",
    "\n",
    "#### Variable Node Update (Vertical Step)\n",
    "The variable nodes aggregate information from all connected check nodes and the original channel observation. Since LLRs are additive, this step is a simple summation:\n",
    "\n",
    "$q_{j \\to i} = L_j^{channel} + \\sum_{k \\in \\mathcal{M}(j) \\setminus i} r_{k \\to j}$\n",
    "\n",
    "#### Tentative Decoding and Termination\n",
    "At the end of each iteration, we compute the total belief $L^{total}_j = L_j^{channel} + \\sum_{k} r_{k \\to j}$. We estimate $\\hat{x}$ based on the sign of $L^{total}$ and check if the syndrome $H\\hat{x} = 0$ is satisfied. \n",
    "\n",
    "If satisfied, we terminate early.\n",
    "\n",
    "---\n",
    "\n",
    "## Optimization\n",
    "To handle matrix operations efficiently, we avoided standard python loops, which run in $O(E)$, where $E$ is the number of edges. Instead, we used Numpy Vectorization\n",
    "\n",
    "Matrix-Based Updates: We maintain message matrices $Q$ and $R$ of size $M \\times N$. Using boolean masking (mask = H == 1), we update all active edges simultaneously using broadcasted operations.\n",
    "\n",
    "The check node update requires the product of all neighbors except the target variable ($\\prod_{k \\neq j}$). Calculating this individually for every edge is computationally expensive.\n",
    "- We compute the product of the entire row once and divide by the specific element for each edge:  \n",
    "\n",
    "    $\\text{Product}_{\\text{excluding } j} = \\frac{\\text{Total Row Product}}{\\text{Element}_j}$\n",
    "\n",
    "*Note: We included a small epsilon to prevent division-by-zero errors during this step.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b89392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding Status: Success\n",
      "Iterations: 8\n",
      "Decoded Vector: [0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1\n",
      " 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
      " 1 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 0\n",
      " 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0\n",
      " 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
      " 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1\n",
      " 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1\n",
      " 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
      " 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0\n",
      " 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
      " 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1\n",
      " 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1\n",
      " 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
      " 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
      " 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1\n",
      " 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ldpc_decoder(H, y, p, max_iter=20):\n",
    "    \"\"\"\n",
    "    Decodes a received LDPC codeword using Loopy Belief Propagation (Sum-Product).\n",
    "    \n",
    "    Args:\n",
    "        H (np.array): Parity check matrix (M x N).\n",
    "        y (np.array): Received vector (size N).\n",
    "        p (float): Noise ratio (probability of flip).\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "        x_hat (np.array): Decoded vector.\n",
    "        status (int): 0 for success, 1 for max iterations reached.\n",
    "        iterations (int): Number of iterations performed.\n",
    "    \"\"\"\n",
    "    M, N = H.shape\n",
    "    \n",
    "    # Initialization of LLR channel\n",
    "    if p == 0: \n",
    "        channel_llr = (1 - 2 * y) * 1e10 \n",
    "    else:\n",
    "        channel_llr = (1 - 2 * y) * np.log((1 - p) / p)\n",
    "    \n",
    "    # Initialize messages \n",
    "    Q = np.zeros((M, N))\n",
    "    R = np.zeros((M, N))\n",
    "    mask = (H == 1)\n",
    "    Q[mask] = np.tile(channel_llr, (M, 1))[mask]\n",
    "    \n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Horizontal Step: Check Node Updates\n",
    "        tanh_Q = np.tanh(Q / 2.0)\n",
    "        tanh_Q = np.clip(tanh_Q, -0.999999, 0.999999)    # Clip for numerical stability\n",
    "        \n",
    "        # Optimized product-excluding-self calculation\n",
    "        safe_tanh = np.where(mask, tanh_Q, 1.0)\n",
    "        row_prod = np.prod(safe_tanh, axis=1)[:, np.newaxis]\n",
    "        denom = safe_tanh + 1e-20 \n",
    "        prod_exclude_self = row_prod / denom\n",
    "        \n",
    "        R[mask] = (2 * np.arctanh(prod_exclude_self))[mask]\n",
    "        \n",
    "        # Vertical Step: Variable Node Updates\n",
    "        total_R_sum = np.sum(R, axis=0)\n",
    "        Q[mask] = (channel_llr + total_R_sum - R)[mask]\n",
    "        \n",
    "        # Tentative Decoding\n",
    "        L_total = channel_llr + total_R_sum\n",
    "        x_hat = (L_total < 0).astype(int)\n",
    "        \n",
    "        # Check Syndrome\n",
    "        syndrome = H @ x_hat % 2\n",
    "        if not np.any(syndrome):\n",
    "            return x_hat, 0, iteration + 1\n",
    "            \n",
    "    return x_hat, -1, max_iter\n",
    "\n",
    "H1 = np.loadtxt('Data/H1.txt', dtype=int)\n",
    "y1 = np.loadtxt('Data/y1.txt', dtype=int)\n",
    "p = 0.1\n",
    "\n",
    "decoded_msg, status, iters = ldpc_decoder(H1, y1, p)\n",
    "\n",
    "print(f\"Decoding Status: {'Success' if status == -1 else 'Failed'}\")\n",
    "print(f\"Iterations: {iters}\")\n",
    "print(f\"Decoded Vector: {decoded_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b731d",
   "metadata": {},
   "source": [
    "**Convergence**: It successfully converged in 8 iterations.\n",
    "\n",
    "**Outcome**: The syndrome check confirmed $Hx = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39777a",
   "metadata": {},
   "source": [
    "# Q3.4 Decode Message\n",
    "\n",
    "To retrieve the message, the first 248 characters are iterated through, with every 8 bits being converted into ASCII\n",
    "\n",
    "Each ASCII character is then appened into a list and returned as a string once all 248 bits have been iterated through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7599c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Message: Happy Holidays! Dmitry&David :)\n"
     ]
    }
   ],
   "source": [
    "def decode_message(decoded_vector):\n",
    "    \"\"\"\n",
    "    Recovers the original English message from the decoded LDPC vector.\n",
    "    \n",
    "    Args:\n",
    "        decoded_vector (np.array): The full decoded output from the LDPC decoder.\n",
    "        \n",
    "    Returns:\n",
    "        str: The recovered ASCII string.\n",
    "    \"\"\"\n",
    "    message_bits = decoded_vector[:248].astype(int)\n",
    "    chars = []\n",
    "    for i in range(0, 248, 8):\n",
    "        byte_chunk = message_bits[i : i+8]\n",
    "        char_code = 0\n",
    "        for bit in byte_chunk:\n",
    "            char_code = (char_code << 1) | bit\n",
    "        chars.append(chr(char_code))\n",
    "        \n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "if status == 0:\n",
    "    secret_message = decode_message(decoded_msg)\n",
    "    print(f\"Original Message: {secret_message}\")\n",
    "else:\n",
    "    print(\"Decoding failed, cannot recover message.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
